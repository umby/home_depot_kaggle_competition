{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 - Home Depot Product Search\n",
    "## Saad, Kevin, & Umber\n",
    "\n",
    "* Link to Kaggle Competition Site: https://www.kaggle.com/c/home-depot-product-search-relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# other useful libraries\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "This section reads in the data from the main two csv files with the proper encoding and converts everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63067, 4) (11000, 4) (166693, 4)\n",
      "(63067,) (11000,)\n"
     ]
    }
   ],
   "source": [
    "# open the train data file\n",
    "with open('train.csv', 'rb') as csvfile:\n",
    "    data_iter = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = [data for data in data_iter]\n",
    "\n",
    "# convert strings to lower case\n",
    "for row in data:\n",
    "    row[2] = row[2].decode('ISO-8859-1').lower()\n",
    "    row[3] = row[3].decode('ISO-8859-1').lower()\n",
    "\n",
    "# load the training data\n",
    "data_array = np.asarray(data)\n",
    "train_size = 63068\n",
    "Y, X = data_array[1:train_size, 4:], data_array[1:train_size, :4]\n",
    "Y_dev, X_dev = data_array[train_size:, 4:], data_array[train_size:, :4]\n",
    "\n",
    "Y = np.reshape(Y, (Y.shape[0],))\n",
    "Y_dev = np.reshape(Y_dev, (Y_dev.shape[0],))\n",
    "\n",
    "train_labels = np.array(Y, dtype=np.float32)\n",
    "dev_labels = np.array(Y_dev, dtype=np.float32)\n",
    "\n",
    "# open the test data\n",
    "with open('test.csv', 'rb') as csvfile:\n",
    "    data_iter = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = [data for data in data_iter]\n",
    "\n",
    "# convert strings to lower case\n",
    "for row in data:\n",
    "    row[2] = row[2].decode('ISO-8859-1').lower()\n",
    "    row[3] = row[3].decode('ISO-8859-1').lower()\n",
    "\n",
    "data_array = np.asarray(data)\n",
    "\n",
    "# load the test data\n",
    "X_test = data_array[1:]\n",
    "\n",
    "print X.shape, X_dev.shape, X_test.shape\n",
    "print Y.shape, Y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Loading\n",
    "\n",
    "The Home Depot data set also includes a group of descriptions and attributes about each product, so this section loads these csv files into dictionaries to aid in feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The data set we were given contains product descriptions as well as attributes for some products\n",
    "# this section will build a dictionary for both of these to use in analysis\n",
    "with open('product_descriptions.csv', 'rb') as csvfile:\n",
    "    data_iter = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = [data for data in data_iter]\n",
    "\n",
    "# dump the headers\n",
    "data = data[1:]\n",
    "\n",
    "pd_dict = {}\n",
    "# build a dictionary from the IDs and descriptions\n",
    "for row in data:\n",
    "    pid = int(row[0])\n",
    "    description = row[1].decode('ISO-8859-1').lower()\n",
    "    pd_dict[pid] = description\n",
    "\n",
    "# Now open the attributes. Note that there may be more that one attribute per ID and there may be zero attributes for some IDs\n",
    "with open('attributes.csv', 'rb') as csvfile:\n",
    "    data_iter = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = [data for data in data_iter]\n",
    "\n",
    "# dump the headers\n",
    "data = data[1:]\n",
    "\n",
    "att_dict = {}\n",
    "# build a dictionary from the attributes\n",
    "for row in data:\n",
    "    # some of the attribute lines are blanks, so check for that and skip\n",
    "    if row[0] != \"\":\n",
    "        pid = int(row[0])\n",
    "        name = row[1].decode('ISO-8859-1').lower()\n",
    "        value = row[2].decode('ISO-8859-1').lower()\n",
    "    \n",
    "    # if this is a new value, add it to the dictionary and add the name value pair as an inner dictionary\n",
    "    if pid not in att_dict:\n",
    "        inner_dict = {}\n",
    "        inner_dict[name] = value\n",
    "        att_dict[pid] = inner_dict\n",
    "    else:\n",
    "        inner_dict = att_dict[pid]\n",
    "        inner_dict[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "First, we defined some helper functions to stem the words and make it easier to count matches and approximate matches. Then we applied these techniques to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2' u'100001' u'simpson strong-tie 12-gauge angle' u'angle bracket'] [ 2.    1.    1.25  1.    0.    0.  ]\n",
      "[u'3' u'100001' u'simpson strong-tie 12-gauge angle' u'l bracket'] [ 2.    1.    0.65  1.    0.    0.  ]\n",
      "(63067, 6)\n"
     ]
    }
   ],
   "source": [
    "# now we need to build the features we're interested in using in our model\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#find the most similar word in string for each word in query and sum the ratios\n",
    "def sum_similar(query, string):\n",
    "    total = 0\n",
    "    for q in query.split():\n",
    "        closest = 0\n",
    "        for s in string.split():\n",
    "            current = SequenceMatcher(None, q, s).ratio()\n",
    "            # print 'sequence matcher ouptut: %s, %s, %3d' %(q, s, current)\n",
    "            if current > closest:\n",
    "                closest = current\n",
    "        total += closest\n",
    "    # print 'sum ratio of \"%s\" and \"%s\" is %3d' %(query, string, total)\n",
    "    return total\n",
    "\n",
    "def find_query_in_string(query, string):\n",
    "    count = 0\n",
    "    for word in query.split():\n",
    "        if string.find(word)>=0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_words(query):\n",
    "    count = 0\n",
    "    for word in query.split():\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def stem_data(string):\n",
    "    s = \"\"\n",
    "    for word in string.split():\n",
    "        s += stemmer.stem(word) + \" \"\n",
    "    return s\n",
    "\n",
    "# start with 4 features:\n",
    "# 1) number of words in query\n",
    "# 2) number of query words present in title\n",
    "# 3) sum of the ratio differences of each query word to the title\n",
    "# 4) number of query words present in description\n",
    "# 5) number of query words present in the MFG Brand attribute\n",
    "# 6) sum of the ratio differences of each query word to the MFG Brand attribute\n",
    "num_features = 6\n",
    "num_query = 0\n",
    "num_title = 1\n",
    "sum_title = 2\n",
    "num_desc = 3\n",
    "num_brand = 4\n",
    "sum_brand = 5\n",
    "\n",
    "train_data = np.zeros((X.shape[0],num_features))\n",
    "\n",
    "# use a snowball stemmer from nltk to remove word endings\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "for x, t in zip(X, train_data):\n",
    "    pid = x[1]\n",
    "    title = stem_data(x[2])\n",
    "    query = stem_data(x[3])\n",
    "    description = stem_data(pd_dict[int(pid)])\n",
    "\n",
    "    t[num_query] = count_words(query)\n",
    "    t[num_title] = find_query_in_string(query, title)\n",
    "    t[sum_title] = sum_similar(query, title)\n",
    "    t[num_desc] = find_query_in_string(query, description)\n",
    "    \n",
    "    if int(pid) in att_dict:\n",
    "        inner_dict = att_dict[int(pid)]\n",
    "        brand_idx = \"MFG Brand Name\"\n",
    "        if brand_idx in inner_dict:\n",
    "            t[num_brand] += find_query_in_string(query, stem_data(inner_dict[brand_idx]))\n",
    "            t[sum_brand] = sum_similar(query, stem_data(inner_dict[brand_idx]))\n",
    "\n",
    "for t in range(2):\n",
    "    print X[t], train_data[t]\n",
    "\n",
    "print train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Feature Engineering\n",
    "\n",
    "Now we do the same thing for the dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'190119' u'181636'\n",
      " u'suntouch floor warming 48 ft. x 30 in. 240 v radiant floor warming mat'\n",
      " u'floor warming matt'] [ 3.          2.          2.85714286  2.          0.          0.        ]\n",
      "[u'190120' u'181637'\n",
      " u'brinks home security 1-13/16 in. (45 mm) laminated steel padlock with 2 in. shackle'\n",
      " u'security padlock'] [ 2.  2.  2.  1.  0.  0.]\n",
      "(11000, 6)\n"
     ]
    }
   ],
   "source": [
    "# now do the same for the dev data\n",
    "dev_data = np.zeros((X_dev.shape[0],num_features))\n",
    "\n",
    "for x, d in zip(X_dev, dev_data):\n",
    "    pid = x[1]\n",
    "    title = stem_data(x[2])\n",
    "    query = stem_data(x[3])\n",
    "    description = stem_data(pd_dict[int(pid)])\n",
    "    \n",
    "    d[num_query] = count_words(query)\n",
    "    d[num_title] = find_query_in_string(query, title)\n",
    "    d[sum_title] = sum_similar(query, title)\n",
    "    d[num_desc] = find_query_in_string(query, description)\n",
    "    \n",
    "    if int(pid) in att_dict:\n",
    "        inner_dict = att_dict[int(pid)]\n",
    "        brand_idx = \"MFG Brand Name\"\n",
    "        if brand_idx in inner_dict:\n",
    "            d[num_brand] += find_query_in_string(query, stem_data(inner_dict[brand_idx]))\n",
    "            d[sum_brand] = sum_similar(query, stem_data(inner_dict[brand_idx]))\n",
    "\n",
    "for t in range(2):\n",
    "    print X_dev[t], dev_data[t]\n",
    "\n",
    "print dev_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Feature Engineering\n",
    "\n",
    "And finally the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1' u'100001' u'simpson strong-tie 12-gauge angle' u'90 degree bracket'] [ 3.          0.          0.47222222  1.          0.          0.        ]\n",
      "[u'4' u'100001' u'simpson strong-tie 12-gauge angle' u'metal l brackets'] [ 3.          1.          1.09444444  1.          0.          0.        ]\n",
      "(166693, 6)\n"
     ]
    }
   ],
   "source": [
    "# Now do the same for the test data\n",
    "test_data = np.zeros((X_test.shape[0],num_features))\n",
    "\n",
    "for x, t in zip(X_test, test_data):\n",
    "    pid = x[1]\n",
    "    title = stem_data(x[2])\n",
    "    query = stem_data(x[3])\n",
    "    description = stem_data(pd_dict[int(pid)])\n",
    "\n",
    "    t[num_query] = count_words(query)\n",
    "    t[num_title] = find_query_in_string(query, title)\n",
    "    t[sum_title] = sum_similar(query, title)\n",
    "    t[num_desc] = find_query_in_string(query, description)\n",
    "    \n",
    "    if int(pid) in att_dict:\n",
    "        inner_dict = att_dict[int(pid)]\n",
    "        brand_idx = \"MFG Brand Name\"\n",
    "        if brand_idx in inner_dict:\n",
    "            t[num_brand] += find_query_in_string(query, stem_data(inner_dict[brand_idx]))\n",
    "            t[sum_brand] = sum_similar(query, stem_data(inner_dict[brand_idx]))\n",
    "\n",
    "for t in range(2):\n",
    "    print X_test[t], test_data[t]\n",
    "\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model\n",
    "\n",
    "Trained a KNN model as a first attempt to get something in a submittable format. Never performed above 24%, which makes sense given what we learned later about our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k is:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/python2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#fit a knn classifier\n",
    "k_values = np.arange(1, 21, 2)\n",
    "knn_f1_scores = np.zeros(k_values.shape)\n",
    "for i in range(k_values.shape[0]):\n",
    "    clf = KNeighborsClassifier(n_neighbors = k_values[i])\n",
    "    clf.fit(train_data, Y)\n",
    "    preds = clf.predict(dev_data)\n",
    "    knn_f1_scores[i] = metrics.f1_score(Y_dev, preds, average='weighted')\n",
    "\n",
    "k_val = k_values[np.argmax(knn_f1_scores)]\n",
    "print 'Optimal value of k is: ', k_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 0.24\n",
      "(166693, 4)\n",
      "(166693,)\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 11)\n",
    "clf.fit(train_data, Y)\n",
    "preds = clf.predict(test_data)\n",
    "\n",
    "print 'KNN accuracy: %3.2f' %clf.score(dev_data, Y_dev)\n",
    "\n",
    "print X_test.shape\n",
    "print preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Tried LR with a variety of parameters with minimal success. Since we learn later that our data is just not linearly separable, LR had no chance of being relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types dtype('<U147') dtype('<U147') dtype('<U147')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-11912e01848a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_labels_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_dev\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_labels_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdev_labels_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_labels_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types dtype('<U147') dtype('<U147') dtype('<U147')"
     ]
    }
   ],
   "source": [
    "train_labels_log = Y * 3 - 3\n",
    "dev_labels_log = Y_dev * 3 - 3\n",
    "\n",
    "train_labels_log = np.array(train_labels_log, dtype=int)\n",
    "dev_labels_log = np.array(dev_labels_log, dtype=int)\n",
    "\n",
    "clf = LogisticRegression(C=0.1)\n",
    "clf.fit(train_data, train_labels_log)\n",
    "preds = clf.predict(dev_data)\n",
    "print metrics.f1_score(dev_labels_log, preds, average='weighted')\n",
    "\n",
    "preds = clf.predict(test_data)\n",
    "\n",
    "log_preds = np.array(preds, dtype=np.float32)\n",
    "\n",
    "log_preds = (log_preds + 3)/3\n",
    "\n",
    "print log_preds.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Tried various decision trees with various boosting techniques, but was never able to achieve much better than KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, Y)\n",
    "\n",
    "print 'Accuracy (a decision tree):', dt.score(dev_data, Y_dev)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_data, Y)\n",
    "\n",
    "print 'Accuracy (a random forest):', rfc.score(dev_data, Y_dev)\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, Y)\n",
    "print 'Accuracy (adaboost with decision trees):', abc.score(dev_data, Y_dev)\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=100), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, Y)\n",
    "print 'Accuracy (adaboost with random forest):', abc.score(dev_data, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Regressor\n",
    "\n",
    "Started to show some promise with the bagging regressor, but still not able to beat the mean baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "num_e = np.arange(10,500,5)\n",
    "score = np.zeros(num_e.shape)\n",
    "\n",
    "for e, i in zip(num_e, range(score.shape[0])):\n",
    "    br = BaggingRegressor(base_estimator=None, n_estimators=e)\n",
    "    br.fit(train_data, Y)\n",
    "    score[i] = br.score(dev_data, Y_dev)\n",
    "    \n",
    "    print 'BaggingRegressor n_estimators: %3d score: %4f' %(e, score[i])\n",
    "    \n",
    "print 'Best score is: ', np.amax(score)\n",
    "print 'Num estimators: ', num_e[np.argmax(score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "Showed some promise, but was never able to beat the mean baseline by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline, grid_search\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "num_e = np.arange(10,500,5)\n",
    "score = np.zeros(num_e.shape)\n",
    "\n",
    "for e, i in zip(num_e, range(score.shape[0])):\n",
    "    rfr = RandomForestRegressor(n_jobs = -1, n_estimators=e)\n",
    "    rfr.fit(train_data, Y)\n",
    "\n",
    "    score[i] = rfr.score(dev_data, Y_dev)\n",
    "    \n",
    "    print 'RandomForestRegressor n_estimators: %3d score: %4f' %(e, score[i])\n",
    "\n",
    "print 'Best score is: ', np.amax(score)\n",
    "print 'Num estimators: ', num_e[np.argmax(score)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs = -1, n_estimators=135)\n",
    "rfr.fit(train_data, Y)\n",
    "\n",
    "preds = rfr.predict(test_data)\n",
    "\n",
    "for i in range(10):\n",
    "    print X_test[i], preds[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Regressor with Random Forest Regressors\n",
    "\n",
    "Finally able to beat the mean baseline with a bunch of bagging regressors with RFs instead of the default decsion trees. This ended up being our top performing model, though we tried several others as you see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "br_est = [100,150,200,250,300,350]\n",
    "\n",
    "for b in br_est:\n",
    "    rf = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "    clf = BaggingRegressor(base_estimator=rf, n_estimators=b, max_samples=0.1, random_state=10)\n",
    "    clf.fit(train_data, Y)\n",
    "\n",
    "\n",
    "    score = clf.score(dev_data, Y_dev)\n",
    "\n",
    "    print 'BaggingRegressor n_estimators: %3d RF estimators: %3d score: %4f' %(b, 20, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_est = 20\n",
    "br_est = 300\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=rf_est, random_state=0)\n",
    "clf = BaggingRegressor(base_estimator=rf, n_estimators=br_est, max_samples=0.1, random_state=10)\n",
    "clf.fit(train_data, Y)\n",
    "preds = clf.predict(test_data)\n",
    "score = clf.score(dev_data, Y_dev)\n",
    "\n",
    "print 'BaggingRegressor n_estimators: %3d RF estimators: %3d score: %4f' %(br_est, rf_est, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Tried a variety of NN techniques, but once again none were able to beat the mean baseline. Perhaps deeper nets would have worked better, but I lacked the computational power to go much further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "print theano.config.device # We're using CPUs (for now)\n",
    "print theano.config.floatX # Should be 64 bit for CPUs\n",
    "\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,13))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "\n",
    "Y_b = Y*6 - 5.95\n",
    "Y_dev_b = Y_dev*6 - 5.95\n",
    "\n",
    "Y_b = np.array(Y_b, dtype=int)\n",
    "Y_dev_b = np.array(Y_dev_b, dtype=int)\n",
    "\n",
    "train_labels_b = binarizeY(Y_b)\n",
    "dev_labels_b = binarizeY(Y_dev_b)\n",
    "numClasses = train_labels_b[1].size\n",
    "numFeatures = train_data[1].size\n",
    "print 'Classes = %d' %(numClasses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parms\n",
    "numHiddenNodes = 1000 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
    "w_4 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2, w_3, w_4]\n",
    "\n",
    "## (2) Model\n",
    "Xm = T.matrix()\n",
    "Ym = T.matrix()\n",
    "def model(Xm, w_1, w_2, w_3, w_4):\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(Xm, w_1)), w_2)), w_3)), w_4))\n",
    "y_hat = model(Xm, w_1, w_2, w_3, w_4)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Ym))\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[Xm, Ym], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[Xm], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(dev_labels_b, axis=1) == predict(dev_data)))\n",
    "    print 'train time = %.2f' %(trainTime)\n",
    "\n",
    "gradientDescentStochastic(500)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print 'predict time = %.2f' %(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM and PCA\n",
    "\n",
    "After a little data exploration on the different classes we had in the labelled data, we finally get a look at how muddled these data are. PCA captures >90% of the variance in 2 components, and you can see from the plot that clustering will not produce very good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print min(Y), max(Y)\n",
    "\n",
    "Y1 = Y[Y==1]\n",
    "Y125 = Y[Y==1.25]\n",
    "Y133 = Y[Y==1.33]\n",
    "Y15 = Y[Y==1.5]\n",
    "Y167 = Y[Y==1.67]\n",
    "Y175 = Y[Y==1.75]\n",
    "Y2 = Y[Y==2]\n",
    "Y225 = Y[Y==2.25]\n",
    "Y233 = Y[Y==2.33]\n",
    "Y25 = Y[Y==2.5]\n",
    "Y267 = Y[Y==2.67]\n",
    "Y275 = Y[Y==2.75]\n",
    "Y3 = Y[Y==3]\n",
    "\n",
    "#for i in range(100):\n",
    "#   print Y[i]\n",
    "\n",
    "plt.hist(Y, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "print Y.shape\n",
    "print Y1.shape[0] + Y125.shape[0] + Y133.shape[0] + Y15.shape[0] + Y167.shape[0] + Y175.shape[0] + Y2.shape[0] + Y225.shape[0] + Y233.shape[0] + Y25.shape[0] + Y267.shape[0] + Y275.shape[0] + Y3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(train_data)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=clusters)\n",
    "Yhat = km.fit_predict(X)\n",
    "\n",
    "# Plot the training data\n",
    "plt.figure(clusters, figsize=(10,10))\n",
    "plt.scatter(X[:,0], X[:,1], c=Y)\n",
    "plt.title('Depot Data Clustered by KMeans n=%2d' %clusters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM and PCA continued\n",
    "\n",
    "And indeed, they did not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GMM\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(train_data)\n",
    "test_data_X = pca.transform(test_data)\n",
    "\n",
    "buckets = [1, 1.25, 1.33, 1.5, 1.67, 1.75, 2, 2.25, 2.33, 2.5, 2.67, 1.75, 3]\n",
    "\n",
    "train_data1 = X[Y==buckets[0]]\n",
    "train_data2 = X[Y==buckets[1]]\n",
    "train_data3 = X[Y==buckets[2]]\n",
    "train_data4 = X[Y==buckets[3]]\n",
    "train_data5 = X[Y==buckets[4]]\n",
    "train_data6 = X[Y==buckets[5]]\n",
    "train_data7 = X[Y==buckets[6]]\n",
    "train_data8 = X[Y==buckets[7]]\n",
    "train_data9 = X[Y==buckets[8]]\n",
    "train_data10 = X[Y==buckets[9]]\n",
    "train_data11 = X[Y==buckets[10]]\n",
    "train_data12 = X[Y==buckets[11]]\n",
    "train_data13 = X[Y==buckets[12]]\n",
    "\n",
    "num_comps = 2\n",
    "#fit 13 GMM models, one for each bucket\n",
    "clf_1 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_1.fit(train_data1)\n",
    "clf_2 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_2.fit(train_data2)\n",
    "clf_3 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_3.fit(train_data3)\n",
    "clf_4 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_4.fit(train_data4)\n",
    "clf_5 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_5.fit(train_data5)\n",
    "clf_6 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_6.fit(train_data6)\n",
    "clf_7 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_7.fit(train_data7)\n",
    "clf_8 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_8.fit(train_data8)\n",
    "clf_9 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_9.fit(train_data9)\n",
    "clf_10 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_10.fit(train_data10)\n",
    "clf_11 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_11.fit(train_data11)\n",
    "clf_12 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_12.fit(train_data12)\n",
    "clf_13 = GMM(n_components=num_comps, covariance_type='full')\n",
    "clf_13.fit(train_data13)\n",
    "\n",
    "#calculate scores for both the positive and the negative GMM models for the 2D projected test data\n",
    "scores1 = clf_1.score(test_data_X)\n",
    "scores2 = clf_2.score(test_data_X)\n",
    "scores3 = clf_3.score(test_data_X)\n",
    "scores4 = clf_4.score(test_data_X)\n",
    "scores5 = clf_5.score(test_data_X)\n",
    "scores6 = clf_6.score(test_data_X)\n",
    "scores7 = clf_7.score(test_data_X)\n",
    "scores8 = clf_8.score(test_data_X)\n",
    "scores9 = clf_9.score(test_data_X)\n",
    "scores10 = clf_10.score(test_data_X)\n",
    "scores11 = clf_11.score(test_data_X)\n",
    "scores12 = clf_12.score(test_data_X)\n",
    "scores13 = clf_13.score(test_data_X)\n",
    "\n",
    "scores = np.array(zip(scores1, scores2, scores3, scores4, scores5, scores6, scores7, scores8, scores9, scores10, scores11, scores12, scores13))\n",
    "\n",
    "scores = np.exp(scores)\n",
    "\n",
    "n_scores = np.zeros(scores.shape)\n",
    "#normalize the scores:\n",
    "for i in range(scores.shape[0]):\n",
    "    total = np.sum(scores[i])\n",
    "    \n",
    "    for j in range(scores.shape[1]):\n",
    "        n_scores[i][j] = scores[i][j] / total\n",
    "\n",
    "for i in range(10):\n",
    "    print scores[i], n_scores[i]\n",
    "\n",
    "preds = np.dot(n_scores, buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print min(preds), max(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Output\n",
    "\n",
    "Running this cell would output the preds array to a csv file in the proper format for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lab_f = open(\"test_labeled.csv\", \"w\") # you will need to edit this directory\n",
    "\n",
    "test_lab_f.write(\"\\\"id\\\",\\\"relevance\\\"\\n\")\n",
    "                 \n",
    "for pred, pid in zip(preds, X_test):\n",
    "    test_lab_f.write(str(pid[0]) + \",\" + str(pred) + \"\\n\")\n",
    "\n",
    "test_lab_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes w/ TF-IDF Weighting\n",
    "\n",
    "Different code base :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# other useful libraries\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#nltk.download()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open the train data file\n",
    "with open('train.csv', 'rb') as csvfile:\n",
    "    fh = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = np.asarray([row for row in fh])\n",
    "    df1 = pd.DataFrame({\n",
    "            'pid': data[:,1],\n",
    "            'score': data[:,4],\n",
    "            'query': data[:,3],\n",
    "            'title': data[:,2]\n",
    "        }, index=data[:,0])\n",
    "\n",
    "with open('product_descriptions.csv', 'rb') as csvfile:\n",
    "    fh = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = np.asarray([row for row in fh])\n",
    "    df2 = pd.DataFrame({\n",
    "            'desc': data[1:,1]\n",
    "        }, index=data[1:,0])\n",
    "\n",
    "with open('attributes.csv', 'rb') as csvfile:\n",
    "    fh = csv.reader(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    data = np.asarray([row for row in fh])\n",
    "    df3 = pd.DataFrame({\n",
    "            'attr_name': data[:,1],\n",
    "            'attr_value': data[:,2]\n",
    "        }, index=data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df1.merge(df2,left_on='pid',right_index=True)\n",
    "df['text'] = df['title'] + ' ' + df['desc']\n",
    "dev_data = df[1:10001]\n",
    "train_data = df[10001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanDoc(doc):\n",
    "    # identify stopwords\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    # use a snowball stemmer from nltk to remove word endings\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    #Remove punctuation,convert lower case and split into seperate words\n",
    "    tokens = re.findall(r\"<a.*?/a>|<[^\\>]*>|[\\w'@#]+\", doc.lower() ,flags = re.UNICODE | re.LOCALE)\n",
    "    #Remove stopwords and words < 2\n",
    "    clean = [token for token in tokens if token not in stopset and len(token) > 2]\n",
    "    #Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in clean]\n",
    "    final = ' '.join(str(x) for x in stemmed)\n",
    "    return final\n",
    "\n",
    "train_query = map(lambda x: cleanDoc(x.decode('ISO-8859-1')),train_data['query'])\n",
    "train_text = map(lambda x: cleanDoc(x.decode('ISO-8859-1')),train_data['text'])\n",
    "# Since Multinomial Naive Bayes expects integers, we must round our data. This also acts as a form of regularization.\n",
    "train_score = map(lambda x: int(round(float(x))),train_data['score'])\n",
    "\n",
    "dev_query = map(lambda x: cleanDoc(x.decode('ISO-8859-1')),dev_data['query'])\n",
    "dev_text = map(lambda x: cleanDoc(x.decode('ISO-8859-1')),dev_data['text'])\n",
    "dev_score = map(lambda x: int(round(float(x))),dev_data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_words(query, string):\n",
    "    matched_words = ''\n",
    "    for word in query.lower().split():\n",
    "        for k in range(string.lower().count(word)):\n",
    "            matched_words += ' '+word\n",
    "    return matched_words[1:]\n",
    "\n",
    "v1 = TfidfVectorizer(min_df=1)\n",
    "train = v1.fit_transform(map(lambda x: match_words(x[0],x[1]),zip(train_query,train_text)))\n",
    "\n",
    "dev = v1.transform(map(lambda x: match_words(x[0],x[1]),zip(dev_query,dev_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1 = TfidfVectorizer(min_df=1)\n",
    "train = v1.fit_transform(map(lambda x: match_words(x[0],x[1]),zip(train_query,train_text)))\n",
    "\n",
    "dev = v1.transform(map(lambda x: match_words(x[0],x[1]),zip(dev_query,dev_text)))\n",
    "\n",
    "a_vec = [0.0001,0.001,0.01,0.1,0.5,0.8,1.1,2,5,10]\n",
    "acc_vec = []\n",
    "rmse_vec = []\n",
    "for a in a_vec:\n",
    "    mnb = MultinomialNB(alpha=a)\n",
    "    mnb.fit(train,train_score)\n",
    "    acc_vec.append(metrics.accuracy_score(dev_score,mnb.predict(dev)))\n",
    "    rmse_vec.append(metrics.mean_squared_error(dev_score,mnb.predict(dev))**0.5)\n",
    "    \n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(a_vec,acc_vec)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(a_vec,rmse_vec)\n",
    "\n",
    "print 'Best Model: alpha = ', a_vec[np.argmin(rmse_vec)], ' RMSE: ', np.amin(rmse_vec), ' Accuracy: ', acc_vec[np.argmin(rmse_vec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
